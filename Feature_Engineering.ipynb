{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is a parameter?\n",
        "\n",
        "In feature engineering, a parameter is a value that influences how raw data is transformed into features.\n",
        "There are two main types: manual parameters and learned parameters.\n",
        "Manual parameters are set by the user, such as the number of bins in discretization.\n",
        "Learned parameters are calculated from the data, like the mean and standard deviation in normalization.\n",
        "Both types help shape the quality and usefulness of the engineered features for machine learning models.\n",
        "\n",
        "#2.What is correlation?What does negative correlation mean?\n",
        "Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "It ranges from -1 to +1:\n",
        "\n",
        "+1 means a perfect positive relationship (both increase together),\n",
        "\n",
        "0 means no relationship,\n",
        "\n",
        "-1 means a perfect negative relationship.\n",
        "\n",
        "What Does Negative Correlation Mean?\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "For example, if the number of hours spent watching TV goes up, test scores might go down — that’s a negative correlation.\n",
        "The closer the value is to -1, the stronger the inverse relationship.\n",
        "\n",
        "#3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Definition of Machine Learning:\n",
        "Machine Learning (ML) is a branch of artificial intelligence that enables computers to learn from data and make decisions or predictions without being explicitly programmed.\n",
        "\n",
        "Main Components of Machine Learning:\n",
        "\n",
        "Data:\n",
        "Raw input used to train and test the model (e.g., images, text, numbers).\n",
        "\n",
        "Features:\n",
        "Meaningful variables extracted from raw data to help the model learn patterns.\n",
        "\n",
        "Model:\n",
        "A mathematical algorithm that learns from data (e.g., decision tree, neural network).\n",
        "\n",
        "Training:\n",
        "The process of feeding data to the model so it can learn patterns.\n",
        "\n",
        "Evaluation:\n",
        "Testing how well the model performs using metrics (e.g., accuracy, precision).\n",
        "\n",
        "Prediction:\n",
        "Using the trained model to make decisions on new, unseen data.\n",
        "\n",
        "#4.How does loss value help in determining whether the model is good or not?\n",
        "The loss value measures how well a machine learning model's predictions match the actual target values.\n",
        "\n",
        "How it helps:\n",
        "\n",
        "Indicates Error:\n",
        "A lower loss means the model's predictions are close to the true values — it’s performing well.\n",
        "A higher loss means the predictions are off — the model needs improvement.\n",
        "\n",
        "Guides Training:\n",
        "During training, the model adjusts its parameters to minimize the loss, improving accuracy over time.\n",
        "\n",
        "Comparison Tool:\n",
        "You can compare loss values across models or training epochs to see which one performs better.\n",
        "\n",
        "In short: Low loss = better model performance (in general).\n",
        "\n",
        "#5.What are continuous and categorical variables?\n",
        "\n",
        "Continuous and Categorical Variables:\n",
        "\n",
        "Continuous Variables:\n",
        "\n",
        "These are numeric values that can take any value within a range.They are measurable and often include decimals.\n",
        "Examples: height, weight, temperature, age.\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "These represent distinct groups or categories.\n",
        "They can be labels (non-numeric) or encoded numbers.\n",
        "Examples: gender, color, country, product type.\n",
        "\n",
        "In short:\n",
        "\n",
        "Continuous = numbers with infinite possibilities\n",
        "\n",
        "Categorical = labeled groups or categories\n",
        "\n",
        "#6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "Handling Categorical Variables in Machine Learning:\n",
        "Categorical variables need to be converted into numerical form before feeding into ML models.\n",
        "\n",
        "Common Techniques>>\n",
        "\n",
        "1. Label Encoding:\n",
        "\n",
        "Converts each category into a unique integer.\n",
        "Example: Red = 0, Green = 1, Blue = 2\n",
        "Used when categories have ordinal (ordered) relationships.\n",
        "\n",
        "2. One-Hot Encoding:\n",
        "\n",
        "Creates a new binary column for each category.\n",
        "Example: Color = Red >> [1, 0, 0] for Red, [0, 1, 0] for Green, etc.\n",
        "Best for nominal (unordered) categories.\n",
        "\n",
        "3. Ordinal Encoding:\n",
        "\n",
        "\n",
        "Assigns ordered numbers to categories based on their rank or level.\n",
        "Example: Low = 1, Medium = 2, High = 3\n",
        "\n",
        "4. Target Encoding:\n",
        "\n",
        "Replaces categories with the mean of the target variable for each category.\n",
        "Can be useful but may lead to overfitting.\n",
        "\n",
        "5. Frequency Encoding:\n",
        "\n",
        "Replaces categories with the frequency of their occurrence in the data.\n",
        "\n",
        "#7.What do you mean by training and testing a dataset?\n",
        "\n",
        "Training and Testing a Dataset\n",
        "In machine learning, the dataset is usually split into two parts: training and testing.\n",
        "\n",
        "1. Training Dataset\n",
        "This part is used to train the model — that means teaching the model to learn patterns from the data.\n",
        "\n",
        "The model adjusts its parameters using this data.\n",
        "\n",
        "2. Testing Dataset\n",
        "This part is used to evaluate how well the model performs on new, unseen data.\n",
        "\n",
        "It helps measure the model’s accuracy and ability to generalize.\n",
        "\n",
        "Why This Split Is Important:\n",
        "\n",
        "Using separate data for testing ensures that the model is not just memorizing the training data but can actually predict well on new data.\n",
        "\n",
        "A common split ratio is 80% training and 20% testing.\n",
        "\n",
        "#8. What is sklearn.preprocessing?\n",
        "sklearn.preprocessing is a module in the scikit-learn library that provides tools to transform and prepare data before feeding it into machine learning models.\n",
        "\n",
        "What it does:\n",
        "\n",
        "Scaling: Adjust feature values (e.g., StandardScaler, MinMaxScaler) to a common range or distribution.\n",
        "\n",
        "Encoding: Convert categorical variables into numeric form (e.g., OneHotEncoder, LabelEncoder).\n",
        "\n",
        "Normalization: Adjust data so features have unit norm (e.g., Normalizer).\n",
        "\n",
        "Imputation: Fill in missing values (e.g., SimpleImputer).\n",
        "\n",
        "Generating polynomial features: Create interaction or polynomial terms (PolynomialFeatures).\n",
        "\n",
        "#9.What is a Test set?\n",
        "A Test set is a portion of your dataset that is kept separate from the training data and is used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "It contains new, unseen data that the model hasn’t learned from. By testing on this set, you can measure how well the model generalizes to real-world data and assess its accuracy, errors, or other metrics.\n",
        "\n",
        "In short:\n",
        "The test set helps check if the model works well beyond the data it was trained on.\n",
        "\n",
        "#10.How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "\n",
        "By using scikit-learn's train_test_split function..20% of data is for testing, 80% for training.\n",
        "\n",
        "How I Approach a Machine Learning Problem\n",
        "Understand the problem and define goals.\n",
        "\n",
        "1. Collect and explore data (data analysis and visualization).\n",
        "\n",
        "2. Clean and preprocess data (handle missing values, encode categories, scale features).\n",
        "\n",
        "3. Split data into training and testing sets.\n",
        "\n",
        "4. Choose a model based on the problem type (classification, regression).\n",
        "\n",
        "5. Train the model on the training data.\n",
        "\n",
        "6. Evaluate the model on the test data using appropriate metrics.\n",
        "\n",
        "7. Tune hyperparameters to improve performance.\n",
        "\n",
        "8. Validate the model (cross-validation or additional test data).\n",
        "\n",
        "9. Deploy the model and monitor its performance over time.\n",
        "\n",
        "#11.Why do we have to perform EDA before fitting a model to the data?\n",
        "We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us:\n",
        "\n",
        "1. Understand the data — its structure, patterns, and relationships.\n",
        "\n",
        "2. Identify data quality issues like missing values, outliers, or errors.\n",
        "\n",
        "3. Choose the right features and preprocessing steps based on insights.\n",
        "\n",
        "4. Select appropriate models by understanding variable types and distributions.\n",
        "\n",
        "5. Prevent mistakes like using irrelevant or noisy data, which can hurt model performance.\n",
        "\n",
        "In short, EDA ensures you build better, more reliable machine learning models by knowing the data well first.\n",
        "\n",
        "#12.What is correlation?\n",
        "Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "It ranges from -1 to +1:\n",
        "\n",
        "+1 means a perfect positive relationship (both increase together),\n",
        "\n",
        "0 means no relationship,\n",
        "\n",
        "-1 means a perfect negative relationship.\n",
        "\n",
        "#13.What does negative correlation mean?\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "For example, if the number of hours spent watching TV goes up, test scores might go down — that’s a negative correlation.\n",
        "The closer the value is to -1, the stronger the inverse relationship.\n",
        "\n",
        "#14. How can you find correlation between variables in Python?\n",
        "\n",
        "You can find the correlation between variables in Python using Pandas or NumPy. The most common way is with Pandas’ .corr() method.\n",
        "\n",
        "example using Pandas:\n",
        "\n",
        "     import pandas as pd\n",
        "\n",
        "\n",
        "    data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 3, 4, 5, 6]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = df.corr()\n",
        "\n",
        "    print(correlation_matrix)\n",
        "\n",
        "\n",
        "correlation between just two variables:\n",
        "\n",
        "    corr_AB = df['A'].corr(df['B'])\n",
        "    print(corr_AB)\n",
        "\n",
        "\n",
        "#15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation means that one event directly causes another — a change in one variable leads to a change in another.\n",
        "\n",
        "\n",
        "Difference Between Correlation and Causation:\n",
        "\n",
        "1. Correlation means two variables are related or move together, but it doesn’t mean one causes the other. They just happen to change at the same time.\n",
        "\n",
        "2. Causation means one variable actually causes the other to change.\n",
        "\n",
        "Example:\n",
        "\n",
        "Correlation: Ice cream sales and shark attacks both increase during summer. They are correlated because they happen at the same time, but buying ice cream doesn’t cause shark attacks. The real cause is the hot weather, which increases both.\n",
        "\n",
        "Causation: Smoking causes lung cancer. Here, smoking directly increases the risk of lung cancer — changing smoking habits affects cancer rates.\n",
        "\n",
        "So, correlation shows a relationship, but causation shows a direct cause-effect link.\n",
        "\n",
        "#16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm used in machine learning and deep learning to adjust the model’s parameters (like weights) during training to minimize the loss function. Its goal is to find the best parameters that reduce prediction errors.\n",
        "\n",
        "Different Types of Optimizers:\n",
        "1. Gradient Descent (GD)\n",
        "\n",
        "  Updates parameters by moving them in the direction of the negative gradient of the loss function.Uses the entire dataset to calculate the gradient at each step.Example: Used in simple linear regression or small datasets.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "   Updates parameters using the gradient calculated from one random data point at a time.Faster but more noisy updates compared to full Gradient Descent.Example: Training large datasets in deep learning models.\n",
        "\n",
        "3. Mini-batch Gradient Descent\n",
        "\n",
        "   A compromise between GD and SGD, it updates parameters using gradients calculated from a small batch of data points.Balances speed and accuracy.Example: Most deep learning frameworks use this by default.\n",
        "\n",
        "4. Momentum\n",
        "\n",
        "   Enhances SGD by adding a fraction of the previous update to the current one, helping accelerate learning and avoid local minima.\n",
        "    Example: Helps train deep neural networks faster.\n",
        "\n",
        "5. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "   Combines ideas of Momentum and RMSProp optimizers, adapting learning rates for each parameter individually using estimates of first and second moments of gradients.\n",
        "\n",
        "    Widely popular for deep learning.\n",
        "\n",
        "    Example: Used in training complex models like CNNs or Transformers.\n",
        "\n",
        "\n",
        "#17.What is sklearn.linear_model?\n",
        "sklearn.linear_model is a module in the scikit-learn library that provides a collection of linear models for regression and classification tasks.\n",
        "\n",
        "What It Contains:\n",
        "1. Linear Regression (e.g., LinearRegression) — predicts continuous target variables using a linear combination of features.\n",
        "\n",
        "2. Logistic Regression (e.g., LogisticRegression) — used for binary or multi-class classification problems.\n",
        "\n",
        "3. Ridge and Lasso Regression — linear models with regularization to prevent overfitting.\n",
        "\n",
        "4. ElasticNet — combines Ridge and Lasso penalties.\n",
        "\n",
        "5. Perceptron and other linear classifiers.\n",
        "\n",
        "These models are simple, fast, interpretable, and work well when the relationship between features and the target is approximately linear.\n",
        "\n",
        "\n",
        "#18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        "What does model.fit() do?\n",
        "\n",
        "The fit() method is used to train a machine learning model. It tells the model to learn patterns from the training data by adjusting its internal parameters based on the input features and target values.\n",
        "\n",
        "What arguments must be given?\n",
        "\n",
        "X: The input data (features), usually a 2D array or DataFrame where each row is a sample and each column is a feature.\n",
        "\n",
        "y: The target values (labels), usually a 1D array or Series with the correct output for each sample in X.\n",
        "\n",
        "#19.What does model.predict() do? What arguments must be given?\n",
        "What does model.predict() do?\n",
        "The predict() method is used after training a model to make predictions on new, unseen data. It takes the input features and returns the model’s predicted output.\n",
        "\n",
        "What arguments must be given?\n",
        "X: The input data (features) for which you want predictions.\n",
        "It should be in the same format and structure as the data used during training.\n",
        "\n",
        "#20.What are continuous and categorical variables?\n",
        "Continuous and Categorical Variables:\n",
        "\n",
        "Continuous Variables:\n",
        "\n",
        "These are numeric values that can take any value within a range.They are measurable and often include decimals.\n",
        "Examples: height, weight, temperature, age.\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "These represent distinct groups or categories.\n",
        "They can be labels (non-numeric) or encoded numbers.\n",
        "Examples: gender, color, country, product type.\n",
        "\n",
        "In short:\n",
        "\n",
        "Continuous = numbers with infinite possibilities\n",
        "\n",
        "Categorical = labeled groups or categories\n",
        "\n",
        "#21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "What is Feature Scaling?\n",
        "\n",
        "Feature scaling is the process of transforming input features so that they are on a similar scale (usually within a specific range like 0 to 1 or -1 to 1).\n",
        "\n",
        "Why It’s Important in Machine Learning:\n",
        "\n",
        "Improves model performance — especially for algorithms that rely on distance or gradient descent (e.g., SVM, KNN, logistic regression).\n",
        "\n",
        "Speeds up training — models converge faster when features are scaled.\n",
        "\n",
        "Prevents bias — without scaling, features with larger ranges can dominate others.\n",
        "\n",
        "Common Methods:\n",
        "\n",
        "1. Min-Max Scaling: Scales values to a range [0, 1].\n",
        "\n",
        "2. Standardization (Z-score): Centers data to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "In short:\n",
        "Feature scaling makes sure all features contribute equally to the model, improving accuracy and training efficiency.\n",
        "\n",
        "#22.How do we perform scaling in Python?\n",
        "\n",
        "\n",
        "scale features using scikit-learn's preprocessing module.\n",
        "\n",
        "1. Min-Max Scaling (Normalization)\n",
        "Scales features to a range between 0 and 1.\n",
        "\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "2. Standardization (Z-score Scaling)\n",
        "Centers features around mean = 0 and standard deviation = 1.\n",
        "\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "Notes:\n",
        "fit_transform() is used to compute the scaling parameters and apply them.\n",
        "\n",
        "Always fit on training data and transform both train and test sets using the same scaler.\n",
        "\n",
        "#23. What is sklearn.preprocessing?\n",
        "\n",
        "\n",
        "sklearn.preprocessing is a module in scikit-learn that provides tools to prepare and transform data before feeding it into a machine learning model.\n",
        "\n",
        "\n",
        "Common Uses:\n",
        "1. Scaling features:StandardScaler, MinMaxScaler\n",
        "   \n",
        "   Ensures features are on the same scale.\n",
        "\n",
        "2. Encoding categorical data: LabelEncoder, OneHotEncoder.\n",
        "\n",
        "   Converts categories into numbers.\n",
        "\n",
        "3. Normalizing data:Normalizer\n",
        "\n",
        "   Scales rows to unit norm (used in text or image processing).\n",
        "\n",
        "4. Handling missing values:SimpleImputer (available in sklearn.impute)\n",
        "\n",
        "5. Generating polynomial features:PolynomialFeatures\n",
        "\n",
        "   Creates interaction terms and higher-degree features.\n",
        "\n",
        "#24.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        " To split data for training and testing in Python, you typically use the\n",
        "\n",
        "    train_test_split function from scikit-learn.\n",
        "    from sklearn.model_selection import train_test_split\n",
        "X = features, y = target variable\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "Parameters:\n",
        "\n",
        "X: Input features\n",
        "\n",
        "y: Target/output labels\n",
        "\n",
        "test_size=0.2: 20% of the data goes to testing, 80% to training\n",
        "\n",
        "random_state=42: Ensures the split is reproducible  \n",
        "\n",
        "\n",
        "#25.Explain data encoding?\n",
        "\n",
        "Data encoding is the process of converting categorical (non-numeric) data into a numeric format so that machine learning models can understand and use it.\n",
        "\n",
        "Why Encoding is Needed:\n",
        "Most machine learning algorithms can’t handle text or categories directly — they need numerical input. So, encoding turns things like \"Red\", \"Blue\" or \"Yes\", \"No\" into numbers.\n",
        "\n",
        "\n",
        "1. Label Encoding:\n",
        "\n",
        "Converts each category into a unique integer.\n",
        "Example: Red = 0, Green = 1, Blue = 2\n",
        "Used when categories have ordinal (ordered) relationships.\n",
        "\n",
        "2. One-Hot Encoding:\n",
        "\n",
        "Creates a new binary column for each category.\n",
        "Example: Color = Red >> [1, 0, 0] for Red, [0, 1, 0] for Green, etc.\n",
        "Best for nominal (unordered) categories.\n",
        "\n",
        "3. Ordinal Encoding:\n",
        "\n",
        "\n",
        "Assigns ordered numbers to categories based on their rank or level.\n",
        "Example: Low = 1, Medium = 2, High = 3\n",
        "\n",
        "4. Target Encoding:\n",
        "\n",
        "Replaces categories with the mean of the target variable for each category.\n",
        "Can be useful but may lead to overfitting.\n",
        "\n",
        "5. Frequency Encoding:\n",
        "\n",
        "Replaces categories with the frequency of their occurrence in the data.\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1mJHQs1jqFPc"
      }
    }
  ]
}