{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Simple Linear Regression\n",
        "Simple Linear Regression (SLR) is a statistical method used to model the relationship between two variables: one independent variable (X) and one dependent variable (Y). The idea is to fit a straight line that best describes how changes in X predict changes in Y. The equation is:\n",
        "Y = mX + c + ε, where\n",
        "\n",
        "Y = dependent variable (output)\n",
        "\n",
        "X = independent variable (input)\n",
        "\n",
        "m = slope (rate of change of Y per unit change in X)\n",
        "\n",
        "c = intercept (value of Y when X = 0)\n",
        "\n",
        "ε = error term (unexplained variation)\n",
        "\n",
        "#2. What are the key assumptions of Simple Linear Regression\n",
        "\n",
        "The assumptions are:\n",
        "\n",
        "Linearity – The relationship between X and Y is linear.\n",
        "\n",
        "Independence – Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity – Constant variance of residuals (errors).\n",
        "\n",
        "Normality of errors – The residuals should follow a normal distribution.\n",
        "\n",
        "No multicollinearity – Not applicable here since only one X variable.\n",
        "\n",
        "#3. What does the coefficient m represent in the equation Y=mX+c\n",
        "\n",
        "The coefficient m (slope) represents how much Y changes for a one-unit increase in X.\n",
        "\n",
        "If m > 0, Y increases with X (positive relationship).\n",
        "\n",
        "If m < 0, Y decreases with X (negative relationship).\n",
        "\n",
        "If m = 0, there is no linear relationship.\n",
        "\n",
        "#4. What does the intercept c represent in the equation Y=mX+c\n",
        "\n",
        "The intercept c is the value of Y when X = 0.\n",
        "\n",
        "It provides the baseline starting point of the regression line.\n",
        "\n",
        "It may or may not have practical meaning depending on context (e.g., \"height when age = 0\" might not be meaningful).\n",
        "\n",
        "It ensures the line fits the observed data properly.\n",
        "\n",
        "#5. How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "#6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "#7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "\n",
        "R² measures the proportion of variance in Y explained by X.\n",
        "\n",
        "Value ranges between 0 and 1.\n",
        "\n",
        "R² = 0 → X explains none of the variation.\n",
        "\n",
        "R² = 1 → X perfectly explains Y.\n",
        "\n",
        "Higher R² indicates better fit, but it doesn’t imply causation.\n",
        "\n",
        "#8. What is Multiple Linear Regression\n",
        "Multiple Linear Regression (MLR) models the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, …, Xn).\n",
        "\n",
        "It helps capture more complex relationships than simple regression.\n",
        "\n",
        "#9. What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "Simple Linear Regression → One independent variable (X).\n",
        "\n",
        "Multiple Linear Regression → Two or more independent variables (X₁, X₂, …).\n",
        "MLR provides more explanatory power but also increases the risk of multicollinearity and overfitting.\n",
        "#10. What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "Linearity between predictors and response.\n",
        "\n",
        "Independence of observations.\n",
        "\n",
        "Homoscedasticity (equal variance of errors).\n",
        "\n",
        "Normal distribution of residuals.\n",
        "\n",
        "No multicollinearity among predictors.\n",
        "\n",
        "No autocorrelation (important in time-series data).\n",
        "#11. What is heteroscedasticity, and how does it affect results\n",
        "\n",
        "Heteroscedasticity occurs when residuals have non-constant variance across levels of X.\n",
        "\n",
        "It violates regression assumptions.\n",
        "\n",
        "Leads to inefficient estimates and biased standard errors.\n",
        "\n",
        "This affects hypothesis testing (t-tests, p-values) making them unreliable.\n",
        "#12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "Remove highly correlated predictors.\n",
        "\n",
        "Use Principal Component Analysis (PCA) or dimensionality reduction.\n",
        "\n",
        "Apply regularization techniques (Ridge, Lasso).\n",
        "\n",
        "Standardize variables to reduce scaling effects.\n",
        "#13. What are some common techniques for transforming categorical variables\n",
        "\n",
        "One-Hot Encoding (Dummy variables) – for nominal categories.\n",
        "\n",
        "Label Encoding – when categories are ordinal.\n",
        "\n",
        "Target Encoding – replaces category with mean target value.\n",
        "\n",
        "Frequency Encoding – replaces category with frequency counts.\n",
        "#14. What is the role of interaction terms in Multiple Linear Regression\n",
        "\n",
        "Interaction terms model the combined effect of two predictors on Y.\n",
        "They capture relationships where the effect of one variable depends on another.\n",
        "#15. How can the interpretation of intercept differ between Simple and Multiple Regression\n",
        "\n",
        "In SLR: Intercept = predicted value of Y when X = 0.\n",
        "\n",
        "In MLR: Intercept = predicted Y when all predictors are 0.\n",
        "Sometimes this is meaningful (income when years of experience = 0), sometimes not.\n",
        "#16. What is the significance of the slope in regression analysis\n",
        "\n",
        "Each slope (coefficient) represents the effect of one predictor on Y, keeping others constant.\n",
        "\n",
        "It quantifies the marginal impact of an independent variable.\n",
        "\n",
        "Helps in prediction and understanding variable importance.\n",
        "\n",
        "#17. What are the limitations of using R² as a sole measure of model performance\n",
        "\n",
        "R² always increases when adding more variables (overfitting risk).\n",
        "\n",
        "It doesn’t indicate causality.\n",
        "\n",
        "A high R² does not mean the model is good (could still have bias).\n",
        "\n",
        "Better to use Adjusted R², AIC, BIC, or RMSE for evaluation.\n",
        "\n",
        "#18. How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "Indicates uncertainty about the estimated coefficient.\n",
        "\n",
        "Suggests that the variable may not significantly affect Y.\n",
        "\n",
        "Could be due to multicollinearity, small sample size, or noisy data.\n",
        "\n",
        "#19. What is polynomial regression\n",
        "\n",
        "Polynomial regression is a type of regression where the relationship between X and Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "\n",
        "It is useful when data shows curvature instead of a straight line.\n",
        "\n",
        "#20. When is polynomial regression used\n",
        "\n",
        "When the relationship between X and Y is nonlinear.\n",
        "\n",
        "In trend analysis (e.g., growth curves).\n",
        "\n",
        "To model diminishing/increasing returns.\n",
        "But beware of overfitting with high-degree polynomials.\n",
        "\n",
        "#21. How does the intercept in regression provide context\n",
        "\n",
        "It anchors the regression line to the Y-axis.\n",
        "\n",
        "Provides baseline prediction when all X = 0.\n",
        "\n",
        "Helps interpret predictions relative to starting conditions.\n",
        "\n",
        "#22. How can heteroscedasticity be identified in residual plots\n",
        "\n",
        "Plot residuals vs predicted values.\n",
        "\n",
        "If variance increases/decreases systematically (fan shape), it indicates heteroscedasticity.\n",
        "\n",
        "Important to fix because it biases standard errors.\n",
        "\n",
        "Fix using log transformations or robust standard errors.\n",
        "\n",
        "#23. What does it mean if a Multiple Linear Regression model has high R² but low Adjusted R²\n",
        "\n",
        "High R² → Model explains variation well.\n",
        "\n",
        "Low Adjusted R² → Extra predictors are not improving the model; they may be irrelevant or adding noise.\n",
        "It signals overfitting.\n",
        "\n",
        "#24. Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        "Scaling puts variables on the same scale.\n",
        "\n",
        "Prevents large-value predictors from dominating the model.\n",
        "\n",
        "Helps improve stability in optimization and regularization (Ridge, Lasso).\n",
        "\n",
        "#25. How does polynomial regression differ from linear regression\n",
        "\n",
        "Linear regression → straight line fit.\n",
        "\n",
        "Polynomial regression → curve fit using powers of X.\n",
        "\n",
        "Both are linear in coefficients, but polynomial allows for non-linear relationships.\n",
        "\n",
        "#27. Can polynomial regression be applied to multiple variables\n",
        "\n",
        "Yes, it becomes Polynomial Multiple Regression\n",
        "It models curved surfaces instead of just curved lines.\n",
        "\n",
        "#28. What are the limitations of polynomial regression\n",
        "\n",
        "Overfitting with high degree.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "Poor extrapolation beyond data range.\n",
        "\n",
        "May require scaling to avoid large coefficient values.\n",
        "\n",
        "#29. What methods can be used to evaluate model fit when selecting degree of polynomial\n",
        "\n",
        "Cross-validation.\n",
        "\n",
        "Adjusted R².\n",
        "\n",
        "AIC/BIC (penalize complexity).\n",
        "\n",
        "RMSE/MSE on test data.\n",
        "\n",
        "Residual plots.\n",
        "\n",
        "#30. Why is visualization important in polynomial regression\n",
        "\n",
        "Helps check if the curve fits the data well.\n",
        "\n",
        "Detects underfitting/overfitting visually.\n",
        "\n",
        "Makes interpretation easier for stakeholders.\n",
        "\n",
        "Residual plots can confirm assumption validity.\n",
        "\n",
        "#31. How is polynomial regression implemented in Python\n",
        "\n",
        "Using scikit-learn:\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    \n",
        "    import numpy as np\n",
        "\n",
        "    X = np.array([1,2,3,4,5]).reshape(-1,1)\n",
        "    y = np.array([2,6,14,28,45])\n",
        "\n",
        "    poly = PolynomialFeatures(degree=2)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "\n",
        "    print(\"Coefficients:\", model.coef_)\n",
        "    print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ALkHR0fsnb42"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fq9v8jDFqJ3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}